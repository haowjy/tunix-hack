{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 00 - SFT Warmup for GRPO Training\n",
    "\n",
    "This notebook trains the model on GSM8K with XML format before GRPO training.\n",
    "\n",
    "**Why SFT warmup?**\n",
    "- GRPO needs variance in rewards to learn\n",
    "- Gemma 3 1B outputs `reasoning:` text format, not `<reasoning>` XML tags\n",
    "- Without SFT, all generations get the same -2.0 reward = no gradient\n",
    "- DeepSeek R1 used \"cold-start data\" (SFT) before RL for the same reason\n",
    "\n",
    "**Target format:**\n",
    "```\n",
    "<reasoning>step-by-step solution</reasoning><answer>final answer</answer>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.8.1\n",
      "Devices: [CudaDevice(id=0)]\n",
      "Default backend: gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 22:11:56.865842  510955 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W1203 22:11:56.867944  510896 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "import qwix\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Tunix imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.models.gemma3 import model as gemma3_model\n",
    "from tunix.models.gemma3 import params_safetensors as gemma3_safetensors\n",
    "from tunix.sft import peft_trainer\n",
    "from tunix.sft import metrics_logger\n",
    "\n",
    "# Our library\n",
    "from tunix_hack.models import load_tokenizer\n",
    "from tunix_hack.inference import create_sampler\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: warmup\n",
      "\n",
      "Configuration loaded.\n",
      "  LoRA rank: 16\n",
      "  Batch size: 1\n",
      "  Max steps: 500\n",
      "  Learning rate: 0.0001\n",
      "Checkpoint directory: /home/jimnix/gitrepos/tunix-hack/outputs/checkpoints/sft/warmup\n"
     ]
    }
   ],
   "source": [
    "# Run configuration\n",
    "RUN_NAME = input(\"Enter run name (e.g., 'warmup', 'sft_v1'): \").strip()\n",
    "if not RUN_NAME:\n",
    "    RUN_NAME = f\"sft_{int(time.time())}\"\n",
    "print(f\"Run name: {RUN_NAME}\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_FAMILY = \"gemma3\"\n",
    "MODEL_VERSION = \"gemma3-1b-it\"\n",
    "\n",
    "# Mesh configuration for single GPU\n",
    "MESH = ((1, 1), (\"fsdp\", \"tp\"))\n",
    "\n",
    "# SFT Training configuration\n",
    "BATCH_SIZE = 1              # Reduced for RTX 3090 memory\n",
    "MAX_SEQ_LENGTH = 768        # Max sequence length (prompt + response)\n",
    "MAX_STEPS = 500             # Brief warmup - just teach format\n",
    "EVAL_EVERY_N_STEPS = 50     # Evaluation frequency\n",
    "NUM_EPOCHS = 1              # Single epoch for warmup\n",
    "TRAIN_FRACTION = 0.9        # Train/val split\n",
    "\n",
    "# Optimizer hyperparameters (higher LR than GRPO)\n",
    "LEARNING_RATE = 1e-4        # Higher than GRPO's 3e-6 for faster format learning\n",
    "WEIGHT_DECAY = 0.01\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)  # 10% warmup\n",
    "\n",
    "# LoRA configuration - same as GRPO for compatibility\n",
    "RANK = 16\n",
    "ALPHA = 32\n",
    "\n",
    "# Inference configuration (for testing)\n",
    "TOTAL_GENERATION_STEPS = 512\n",
    "GENERATION_CONFIGS = {\n",
    "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "}\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\"/home/jimnix/gitrepos/tunix-hack\")\n",
    "SFT_CKPT_DIR = str(PROJECT_ROOT / \"outputs\" / \"checkpoints\" / \"sft\" / RUN_NAME)\n",
    "TENSORBOARD_DIR = str(PROJECT_ROOT / \"tmp\" / \"tensorboard\" / \"sft\" / RUN_NAME)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(SFT_CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(TENSORBOARD_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nConfiguration loaded.\")\n",
    "print(f\"  LoRA rank: {RANK}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Checkpoint directory: {SFT_CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Cell 3: Download Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading google/gemma-3-1b-it from HuggingFace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f2e75b0d314f5390add2c5f8ab7b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: /home/jimnix/.cache/huggingface/hub/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\n",
      "  .gitattributes\n",
      "  special_tokens_map.json\n",
      "  tokenizer.model\n",
      "  model.safetensors\n",
      "  added_tokens.json\n",
      "  README.md\n",
      "  config.json\n",
      "  generation_config.json\n",
      "  tokenizer_config.json\n",
      "  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID} from HuggingFace...\")\n",
    "model_path = snapshot_download(MODEL_ID)\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "for f in Path(model_path).iterdir():\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Cell 4: JAX Mesh Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh created: Mesh('fsdp': 1, 'tp': 1, axis_types=(Auto, Auto))\n",
      "Mesh devices: [[CudaDevice(id=0)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_510896/2100874485.py:1: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh(*MESH)\n"
     ]
    }
   ],
   "source": [
    "mesh = jax.make_mesh(*MESH)\n",
    "print(f\"Mesh created: {mesh}\")\n",
    "print(f\"Mesh devices: {mesh.devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Cell 5: Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "PAD_ID: 0\n",
      "EOS_ID: 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(model_path)\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Get special token IDs\n",
    "PAD_ID = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else 0\n",
    "EOS_ID = 1  # Gemma EOS token\n",
    "\n",
    "print(f\"PAD_ID: {PAD_ID}\")\n",
    "print(f\"EOS_ID: {EOS_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Cell 6: Helper Functions for Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def get_model_config():\n",
    "    \"\"\"Get Gemma 3 1B model configuration.\"\"\"\n",
    "    return gemma3_model.ModelConfig.gemma3_1b()\n",
    "\n",
    "\n",
    "def get_base_model(model_path: str):\n",
    "    \"\"\"Load base Gemma 3 model from HuggingFace safetensors.\"\"\"\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = get_model_config()\n",
    "    \n",
    "    model = gemma3_safetensors.create_model_from_safe_tensors(\n",
    "        model_path,\n",
    "        model_config,\n",
    "        mesh,\n",
    "    )\n",
    "    return model, mesh, model_config\n",
    "\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    \"\"\"Apply LoRA to base model.\"\"\"\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "    \n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, **model_input\n",
    "    )\n",
    "    \n",
    "    return lora_model\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Cell 7: Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_510896/2959729399.py:8: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
      "  mesh = jax.make_mesh(*MESH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base model...\")\n",
    "base_model, mesh, model_config = get_base_model(model_path)\n",
    "print(\"Base model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Cell 8: Create LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LoRA model...\n",
      "LoRA model created. Rank: 16, Alpha: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating LoRA model...\")\n",
    "lora_model = get_lora_model(base_model, mesh=mesh)\n",
    "print(f\"LoRA model created. Rank: {RANK}, Alpha: {ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Cell 9: Load GSM8K and Format for SFT\n",
    "\n",
    "GSM8K has step-by-step solutions. We format them as:\n",
    "- Input: Full prompt with question\n",
    "- Target: `<reasoning>solution</reasoning><answer>N</answer>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K dataset...\n",
      "Train examples: 7473\n",
      "Test examples: 1319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e4ba7159d7426eaf71b5288d5ed000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting train data:   0%|          | 0/7473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41de5483d1c47adbd5b4f334c60e18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting test data:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example formatted data:\n",
      "INPUT:\n",
      "<start_of_turn>user\n",
      "You are a math tutor. Solve the problem step by step.\n",
      "Format your response EXACTLY as:\n",
      "<reasoning>your step-by-step solution</reasoning><answer>final numerical answer only</answer>\n",
      "\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "TARGET:\n",
      "<reasoning>Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.</reasoning><answer>72</answer>\n"
     ]
    }
   ],
   "source": [
    "# System prompt (same as GRPO)\n",
    "SYSTEM_PROMPT = \"\"\"You are a math tutor. Solve the problem step by step.\n",
    "Format your response EXACTLY as:\n",
    "<reasoning>your step-by-step solution</reasoning><answer>final numerical answer only</answer>\"\"\"\n",
    "\n",
    "TEMPLATE = \"<start_of_turn>user\\n{system_prompt}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "\n",
    "def extract_hash_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract final answer from GSM8K format (#### followed by number).\"\"\"\n",
    "    if \"####\" in answer_text:\n",
    "        return answer_text.split(\"####\")[-1].strip()\n",
    "    return answer_text.strip()\n",
    "\n",
    "\n",
    "def extract_reasoning(answer_text: str) -> str:\n",
    "    \"\"\"Extract reasoning (everything before ####).\"\"\"\n",
    "    if \"####\" in answer_text:\n",
    "        return answer_text.split(\"####\")[0].strip()\n",
    "    return answer_text.strip()\n",
    "\n",
    "\n",
    "def format_for_sft(example):\n",
    "    \"\"\"Format a GSM8K example for SFT training.\n",
    "    \n",
    "    Returns dict with 'input' (prompt) and 'target' (XML formatted response).\n",
    "    \"\"\"\n",
    "    question = example[\"question\"]\n",
    "    full_answer = example[\"answer\"]\n",
    "    \n",
    "    # Extract reasoning and answer\n",
    "    reasoning = extract_reasoning(full_answer)\n",
    "    answer = extract_hash_answer(full_answer)\n",
    "    \n",
    "    # Create input prompt\n",
    "    input_text = TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    # Create XML formatted target\n",
    "    target_text = f\"<reasoning>{reasoning}</reasoning><answer>{answer}</answer>\"\n",
    "    \n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"target\": target_text,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "gsm8k_test = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "print(f\"Train examples: {len(gsm8k_train)}\")\n",
    "print(f\"Test examples: {len(gsm8k_test)}\")\n",
    "\n",
    "# Format for SFT\n",
    "train_data = [\n",
    "    format_for_sft({\"question\": ex[\"question\"], \"answer\": ex[\"answer\"]})\n",
    "    for ex in tqdm(gsm8k_train, desc=\"Formatting train data\")\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    format_for_sft({\"question\": ex[\"question\"], \"answer\": ex[\"answer\"]})\n",
    "    for ex in tqdm(gsm8k_test, desc=\"Formatting test data\")\n",
    "]\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample formatted data:\")\n",
    "print(\"INPUT:\")\n",
    "print(train_data[0][\"input\"])\n",
    "print(\"\\nTARGET:\")\n",
    "print(train_data[0][\"target\"][:500] + \"...\" if len(train_data[0][\"target\"]) > 500 else train_data[0][\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Cell 10: Tokenize and Create Datasets\n",
    "\n",
    "Tunix SFT needs tokenized data with:\n",
    "- `input_tokens`: Full sequence (prompt + target)\n",
    "- `input_mask`: 1 for target tokens (compute loss), 0 for prompt tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ade4c526364653a0b6616ec06dde6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/7473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0c78d949f24a2abe3f5a8a95e9a71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets created:\n",
      "  Train batches: 500\n",
      "  Val batches: 50\n",
      "\n",
      "First batch shapes:\n",
      "  input_tokens: (1, 285)\n",
      "  input_mask: (1, 285)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_for_sft(example):\n",
    "    \"\"\"Tokenize example for SFT training.\n",
    "    \n",
    "    Creates:\n",
    "    - input_tokens: Full tokenized sequence (prompt + target + EOS)\n",
    "    - input_mask: 1 for target tokens (loss), 0 for prompt tokens\n",
    "    \"\"\"\n",
    "    # Tokenize prompt and target separately to know where prompt ends\n",
    "    prompt_tokens = tokenizer.encode(example[\"input\"])\n",
    "    target_tokens = tokenizer.encode(example[\"target\"])\n",
    "    \n",
    "    # Combine: prompt + target + EOS\n",
    "    # Note: tokenizer.encode adds BOS, so we skip it for target\n",
    "    full_tokens = prompt_tokens + target_tokens[1:] + [EOS_ID]  # Skip BOS from target\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(full_tokens) > MAX_SEQ_LENGTH:\n",
    "        full_tokens = full_tokens[:MAX_SEQ_LENGTH]\n",
    "    \n",
    "    # Create input_mask: 0 for prompt, 1 for target (where loss is computed)\n",
    "    prompt_len = len(prompt_tokens)\n",
    "    input_mask = [0] * prompt_len + [1] * (len(full_tokens) - prompt_len)\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": np.array(full_tokens, dtype=np.int32),\n",
    "        \"input_mask\": np.array(input_mask, dtype=np.int32),\n",
    "    }\n",
    "\n",
    "\n",
    "def pad_batch(batch, pad_id=PAD_ID):\n",
    "    \"\"\"Pad a batch of examples to the same length.\"\"\"\n",
    "    tokens_list = batch[\"input_tokens\"]\n",
    "    masks_list = batch[\"input_mask\"]\n",
    "\n",
    "    max_len = max(len(t) for t in tokens_list)\n",
    "\n",
    "    padded_tokens = []\n",
    "    padded_masks = []\n",
    "\n",
    "    for tokens, mask in zip(tokens_list, masks_list):\n",
    "        pad_len = max_len - len(tokens)\n",
    "        padded_tokens.append(np.pad(tokens, (0, pad_len), constant_values=pad_id))\n",
    "        padded_masks.append(np.pad(mask, (0, pad_len), constant_values=0))\n",
    "        \n",
    "    return {\n",
    "        \"input_tokens\": np.stack(padded_tokens),\n",
    "        \"input_mask\": np.stack(padded_masks),\n",
    "    }\n",
    "\n",
    "\n",
    "# Tokenize all examples\n",
    "print(\"Tokenizing training data...\")\n",
    "train_tokenized = [\n",
    "    tokenize_for_sft(ex) for ex in tqdm(train_data, desc=\"Tokenizing\")\n",
    "]\n",
    "\n",
    "print(\"Tokenizing test data...\")\n",
    "test_tokenized = [\n",
    "    tokenize_for_sft(ex) for ex in tqdm(test_data, desc=\"Tokenizing\")\n",
    "]\n",
    "\n",
    "# Create grain datasets\n",
    "num_train = int(len(train_tokenized) * TRAIN_FRACTION)\n",
    "train_examples = train_tokenized[:num_train]\n",
    "val_examples = train_tokenized[num_train:]\n",
    "\n",
    "# Limit to MAX_STEPS batches for warmup\n",
    "num_batches = min(MAX_STEPS, len(train_examples) // BATCH_SIZE)\n",
    "\n",
    "train_ds = (\n",
    "    grain.MapDataset.source(train_examples)\n",
    "    .shuffle(seed=42)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(pad_batch)\n",
    "    [:num_batches]\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    grain.MapDataset.source(val_examples)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(pad_batch)\n",
    "    [:50]  # Limit validation batches\n",
    ")\n",
    "\n",
    "print(f\"\\nDatasets created:\")\n",
    "print(f\"  Train batches: {len(train_ds)}\")\n",
    "print(f\"  Val batches: {len(val_ds)}\")\n",
    "\n",
    "# Show first batch shape\n",
    "first_batch = train_ds[0]\n",
    "print(f\"\\nFirst batch shapes:\")\n",
    "print(f\"  input_tokens: {first_batch['input_tokens'].shape}\")\n",
    "print(f\"  input_mask: {first_batch['input_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Cell 11: Define gen_model_input_fn\n",
    "\n",
    "Required by PeftTrainer to format batch for Gemma model forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_model_input_fn defined (using tunix.sft.utils).\n",
      "Test output keys: dict_keys(['input_tokens', 'input_mask', 'positions', 'attention_mask'])\n",
      "  positions shape: (1, 285)\n",
      "  attention_mask shape: (1, 285, 285)\n"
     ]
    }
   ],
   "source": [
    "# Use Tunix's SFT utilities for correct attention mask format\n",
    "from tunix.sft import utils as sft_utils\n",
    "\n",
    "\n",
    "def gen_model_input_fn(x):\n",
    "    \"\"\"Transform training input for Gemma model forward pass.\n",
    "    \n",
    "    Uses tunix.sft.utils for correct position/attention mask formats.\n",
    "    \n",
    "    Args:\n",
    "        x: TrainingInput with input_tokens and input_mask\n",
    "        \n",
    "    Returns:\n",
    "        Dict with keys expected by Gemma model.\n",
    "    \"\"\"\n",
    "    # Handle both TrainingInput namedtuple and dict\n",
    "    if hasattr(x, 'input_tokens'):\n",
    "        input_tokens = x.input_tokens\n",
    "        input_mask = x.input_mask\n",
    "    else:\n",
    "        input_tokens = x['input_tokens']\n",
    "        input_mask = x['input_mask']\n",
    "    \n",
    "    # Convert to JAX arrays if needed\n",
    "    input_tokens = jnp.asarray(input_tokens)\n",
    "    input_mask = jnp.asarray(input_mask)\n",
    "    \n",
    "    # Build padding mask (True where not padding)\n",
    "    pad_mask = input_tokens != PAD_ID\n",
    "    \n",
    "    # Use Tunix's utilities (handles Gemma3 attention format correctly)\n",
    "    positions = sft_utils.build_positions_from_mask(pad_mask)\n",
    "    attention_mask = sft_utils.make_causal_attn_mask(pad_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_tokens': input_tokens,\n",
    "        'input_mask': input_mask,\n",
    "        'positions': positions,\n",
    "        'attention_mask': attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"gen_model_input_fn defined (using tunix.sft.utils).\")\n",
    "\n",
    "# Test it\n",
    "test_input = gen_model_input_fn(train_ds[0])\n",
    "print(f\"Test output keys: {test_input.keys()}\")\n",
    "print(f\"  positions shape: {test_input['positions'].shape}\")\n",
    "print(f\"  attention_mask shape: {test_input['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Cell 12: Configure Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer configured.\n",
      "  Learning rate: 0.0001\n",
      "  Warmup steps: 50\n",
      "  Weight decay: 0.01\n"
     ]
    }
   ],
   "source": [
    "# AdamW optimizer with warmup cosine decay\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(\"Optimizer configured.\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Cell 13: Create PeftTrainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PeftTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimmpanda\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jimnix/gitrepos/tunix-hack/wandb/run-20251203_221249-6y8tvz04</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/immpanda/tunix/runs/6y8tvz04?apiKey=d54b18bf445000f216c0c9372acbd65d4a7df1dd' target=\"_blank\">2025-12-03_22-12-49</a></strong> to <a href='https://wandb.ai/immpanda/tunix?apiKey=d54b18bf445000f216c0c9372acbd65d4a7df1dd' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/immpanda/tunix?apiKey=d54b18bf445000f216c0c9372acbd65d4a7df1dd' target=\"_blank\">https://wandb.ai/immpanda/tunix?apiKey=d54b18bf445000f216c0c9372acbd65d4a7df1dd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/immpanda/tunix/runs/6y8tvz04?apiKey=d54b18bf445000f216c0c9372acbd65d4a7df1dd' target=\"_blank\">https://wandb.ai/immpanda/tunix/runs/6y8tvz04?apiKey=d54b18bf445000f216c0c9372acbd65d4a7df1dd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftTrainer created.\n",
      "  Max steps: 500\n",
      "  Eval every: 50 steps\n",
      "  Checkpoint dir: /home/jimnix/gitrepos/tunix-hack/outputs/checkpoints/sft/warmup\n"
     ]
    }
   ],
   "source": [
    "# Metrics logging\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=TENSORBOARD_DIR,\n",
    "    flush_every_n_steps=20,\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    checkpoint_root_directory=SFT_CKPT_DIR,\n",
    "    metrics_logging_options=metrics_logging_options,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "print(\"Creating PeftTrainer...\")\n",
    "trainer = peft_trainer.PeftTrainer(\n",
    "    lora_model,\n",
    "    optimizer,\n",
    "    training_config,\n",
    ").with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "print(\"PeftTrainer created.\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Eval every: {EVAL_EVERY_N_STEPS} steps\")\n",
    "print(f\"  Checkpoint dir: {SFT_CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training...\n",
      "  Training batches: 500\n",
      "  Validation batches: 50\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0d1d5e62314489b2571656351b89e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 5. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 11. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 13. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    }
   ],
   "source": [
    "# Run training!\n",
    "print(\"Starting SFT training...\")\n",
    "print(f\"  Training batches: {len(train_ds)}\")\n",
    "print(f\"  Validation batches: {len(val_ds)}\")\n",
    "print()\n",
    "\n",
    "with mesh:\n",
    "    trainer.train(train_ds, val_ds)\n",
    "\n",
    "print(\"\\nSFT Training complete!\")\n",
    "print(f\"Checkpoints saved to: {SFT_CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Cell 14: Test Trained Model\n",
    "\n",
    "Verify the model now generates XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampler for inference\n",
    "sampler = create_sampler(\n",
    "    lora_model,\n",
    "    tokenizer,\n",
    "    model_config,\n",
    "    max_cache_size=256 + TOTAL_GENERATION_STEPS + 256,\n",
    ")\n",
    "print(\"Sampler created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is 15 + 27?\",\n",
    "    \"If Mary has 5 apples and gives 2 to John, how many apples does Mary have?\",\n",
    "    \"A store sells 3 books for $12. How much does one book cost?\",\n",
    "]\n",
    "\n",
    "print(\"Testing trained model...\\n\")\n",
    "print(\"Checking if model now generates XML format:\\n\")\n",
    "\n",
    "for question in test_questions:\n",
    "    prompt = TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    result = sampler(\n",
    "        input_strings=[prompt],\n",
    "        max_generation_steps=TOTAL_GENERATION_STEPS,\n",
    "        **GENERATION_CONFIGS[\"greedy\"],\n",
    "        echo=False,\n",
    "        eos_tokens=[1, 106],\n",
    "    )\n",
    "    \n",
    "    response = result.text[0]\n",
    "    \n",
    "    # Check for XML tags\n",
    "    has_reasoning = \"<reasoning>\" in response and \"</reasoning>\" in response\n",
    "    has_answer = \"<answer>\" in response and \"</answer>\" in response\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {response[:500]}\" + (\"...\" if len(response) > 500 else \"\"))\n",
    "    print(f\"Has <reasoning> tags: {has_reasoning}\")\n",
    "    print(f\"Has <answer> tags: {has_answer}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Cell 15: Save Checkpoint Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import os\n",
    "\n",
    "ckpt_dir = Path(SFT_CKPT_DIR)\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = [d for d in ckpt_dir.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "    if checkpoints:\n",
    "        latest = max(checkpoints, key=lambda x: int(x.name))\n",
    "        print(f\"Latest checkpoint: {latest}\")\n",
    "        print(f\"\\nTo use in GRPO notebook, set:\")\n",
    "        print(f'SFT_CKPT_PATH = \"{latest}/model_params\"')\n",
    "    else:\n",
    "        print(\"No checkpoints found!\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {SFT_CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del trainer\n",
    "del sampler\n",
    "gc.collect()\n",
    "print(\"Memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunix-hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
