{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - GRPO Training with Tunix (Local RTX 3090)\n",
    "\n",
    "This notebook implements GRPO (Group Relative Policy Optimization) training for Gemma 3 1B on GSM8K math problems.\n",
    "\n",
    "Based on the official [Tunix GRPO Demo](https://github.com/google/tunix/blob/main/examples/grpo_demo.ipynb), adapted for local GPU execution.\n",
    "\n",
    "**Target format:**\n",
    "```\n",
    "<reasoning>step-by-step solution</reasoning><answer>final answer</answer>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "import qwix\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "# Tunix imports\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import model as gemma3_model\n",
    "from tunix.models.gemma3 import params_safetensors as gemma3_safetensors\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "\n",
    "# Our library\n",
    "from tunix_hack.utils.xml_parsing import extract_tag, has_valid_format\n",
    "from tunix_hack.rewards.math_reward import normalize_math_answer\n",
    "from tunix_hack.models import load_tokenizer\n",
    "from tunix_hack.inference import create_sampler\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configuration\n",
    "RUN_NAME = input(\"Enter run name (e.g., 'run_1', 'baseline', 'rank16'): \").strip()\n",
    "if not RUN_NAME:\n",
    "    RUN_NAME = f\"run_{int(time.time())}\"\n",
    "print(f\"Run name: {RUN_NAME}\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_FAMILY = \"gemma3\"\n",
    "MODEL_VERSION = \"gemma3-1b-it\"  # Instruction-tuned variant\n",
    "\n",
    "# Mesh configuration for single GPU\n",
    "# Gemma 3 requires both fsdp and tp axes, even on single device\n",
    "MESH = ((1, 1), (\"fsdp\", \"tp\"))\n",
    "\n",
    "# Training configuration - aggressive memory optimization for RTX 3090 (24GB VRAM)\n",
    "TRAIN_MICRO_BATCH_SIZE = 1   # Reduced from 4 for memory\n",
    "NUM_BATCHES = 500            # Number of training batches\n",
    "NUM_EPOCHS = 3               # Number of epochs\n",
    "NUM_ITERATIONS = 1           # Optimization iterations per batch\n",
    "TRAIN_FRACTION = 0.9         # Train/val split\n",
    "EVAL_EVERY_N_STEPS = 10      # Evaluation frequency\n",
    "\n",
    "# Calculate training steps dynamically (like reference)\n",
    "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "WARMUP_STEPS = int(0.1 * MAX_STEPS)  # 10% warmup (like reference)\n",
    "\n",
    "# Generation configuration\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "TOTAL_GENERATION_STEPS = 512\n",
    "TEMPERATURE = 0.9            # High-ish for diverse responses during training\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "\n",
    "# GRPO hyperparameters\n",
    "NUM_GENERATIONS = 4          # G in GRPO algorithm\n",
    "BETA = 0.08                  # KL divergence penalty coefficient\n",
    "EPSILON = 0.2                # Clipping range for stable updates\n",
    "\n",
    "# Optimizer hyperparameters\n",
    "LEARNING_RATE = 3e-6         # Peak learning rate\n",
    "WEIGHT_DECAY = 0.1           # AdamW weight decay\n",
    "B1 = 0.9                     # Adam beta1\n",
    "B2 = 0.99                    # Adam beta2\n",
    "MAX_GRAD_NORM = 0.1          # Gradient clipping\n",
    "\n",
    "# LoRA configuration - reduced rank for memory\n",
    "RANK = 16                    # Reduced from 64 for memory (still good quality)\n",
    "ALPHA = 32                   # Usually alpha = rank, but can experiment\n",
    "\n",
    "# Dataset configuration\n",
    "NUM_TEST_BATCHES = 10        # Originally 100, reduced for for memory constraints\n",
    "\n",
    "# Inference generation configs (from reference)\n",
    "GENERATION_CONFIGS = {\n",
    "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "}\n",
    "\n",
    "# Paths - organized by run name (use project dir, not system /tmp)\n",
    "PROJECT_ROOT = Path(\"/home/jimnix/gitrepos/tunix-hack\")\n",
    "CKPT_DIR = str(PROJECT_ROOT / \"outputs\" / \"checkpoints\" / \"grpo\" / RUN_NAME)\n",
    "INTERMEDIATE_CKPT_DIR = str(PROJECT_ROOT / \"tmp\" / \"tunix_intermediate\" / RUN_NAME)\n",
    "TENSORBOARD_DIR = str(PROJECT_ROOT / \"tmp\" / \"tensorboard\" / \"grpo\" / RUN_NAME)\n",
    "\n",
    "# === SFT Warmup Checkpoint Loading ===\n",
    "# Set to True to load LoRA weights from SFT warmup before GRPO training\n",
    "# This helps the model start with XML format knowledge\n",
    "LOAD_SFT_CHECKPOINT = True\n",
    "SFT_CKPT_DIR = str(PROJECT_ROOT / \"outputs\" / \"checkpoints\" / \"sft\")\n",
    "# Auto-find latest SFT checkpoint, or specify manually:\n",
    "# SFT_CKPT_PATH = str(PROJECT_ROOT / \"outputs\" / \"checkpoints\" / \"sft\" / \"warmup\" / \"500\" / \"model_params\")\n",
    "SFT_CKPT_PATH = None  # Will be auto-detected if LOAD_SFT_CHECKPOINT is True\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(INTERMEDIATE_CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(TENSORBOARD_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nConfiguration loaded (memory-optimized for RTX 3090).\")\n",
    "print(f\"  LoRA rank: {RANK}\")\n",
    "print(f\"  Batch size: {TRAIN_MICRO_BATCH_SIZE}\")\n",
    "print(f\"  Generations: {NUM_GENERATIONS}\")\n",
    "print(f\"  MAX_STEPS: {MAX_STEPS} (calculated)\")\n",
    "print(f\"  WARMUP_STEPS: {WARMUP_STEPS} (10% of max)\")\n",
    "print(f\"  Load SFT checkpoint: {LOAD_SFT_CHECKPOINT}\")\n",
    "print(f\"Checkpoint directory: {CKPT_DIR}\")\n",
    "print(f\"TensorBoard directory: {TENSORBOARD_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: HuggingFace Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Gemma 3 1B instruction-tuned model from HuggingFace\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID} from HuggingFace...\")\n",
    "model_path = snapshot_download(MODEL_ID)\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "# List downloaded files\n",
    "for f in Path(model_path).iterdir():\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: JAX Mesh Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JAX mesh for single GPU\n",
    "# For multi-GPU, you would use (num_gpus,) or (fsdp, tp) configuration\n",
    "mesh = jax.make_mesh(*MESH)\n",
    "print(f\"Mesh created: {mesh}\")\n",
    "print(f\"Mesh devices: {mesh.devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = load_tokenizer(model_path)\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Hello, world!\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"Test: '{test_text}' -> {tokens} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Helper Functions for Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config():\n",
    "    \"\"\"Get Gemma 3 1B model configuration.\"\"\"\n",
    "    return gemma3_model.ModelConfig.gemma3_1b()\n",
    "\n",
    "\n",
    "def get_base_model(model_path: str):\n",
    "    \"\"\"Load base Gemma 3 model from HuggingFace safetensors.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the downloaded model directory.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, mesh, model_config).\n",
    "    \"\"\"\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = get_model_config()\n",
    "    \n",
    "    # Load model from safetensors with sharding\n",
    "    model = gemma3_safetensors.create_model_from_safe_tensors(\n",
    "        model_path,\n",
    "        model_config,\n",
    "        mesh,\n",
    "    )\n",
    "    return model, mesh, model_config\n",
    "\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    \"\"\"Apply LoRA to base model to create trainable policy.\n",
    "    \n",
    "    Note: This modifies base_model in-place, allowing backbone sharing\n",
    "    between the LoRA policy and reference model for memory efficiency.\n",
    "    \n",
    "    Args:\n",
    "        base_model: The base Gemma model.\n",
    "        mesh: JAX mesh for sharding (unused, kept for API compatibility).\n",
    "        \n",
    "    Returns:\n",
    "        LoRA-augmented model (shares backbone with base_model).\n",
    "    \"\"\"\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        # Target attention and MLP layers\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "    \n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, **model_input\n",
    "    )\n",
    "    \n",
    "    # Don't re-shard - this preserves shared backbone with base_model\n",
    "    return lora_model\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Load and Convert Base Model\n",
    "\n",
    "**Note:** Loading from HuggingFace safetensors and saving as intermediate checkpoint for NNX compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We now load directly from safetensors in the next cell,\n",
    "# so intermediate checkpoint is not needed for this workflow.\n",
    "# The get_base_model function handles sharding automatically.\n",
    "print(\"Intermediate checkpoint step skipped - loading directly from safetensors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Load Base Model\n",
    "\n",
    "Load directly from HuggingFace safetensors. This model will be used as both the reference (frozen) and the backbone for LoRA (shared weights for memory efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model directly from safetensors (used as both reference and LoRA backbone)\n",
    "# This enables backbone sharing between actor and reference for memory efficiency\n",
    "print(\"Loading base model...\")\n",
    "base_model, mesh, model_config = get_base_model(model_path)\n",
    "print(\"Base model loaded.\")\n",
    "nnx.display(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Create LoRA Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA-augmented policy model (trainable)\n",
    "# Note: This shares the backbone with base_model for memory efficiency\n",
    "print(\"Creating LoRA policy model...\")\n",
    "lora_policy = get_lora_model(base_model, mesh=mesh)\n",
    "print(\"LoRA policy model created.\")\n",
    "print(f\"LoRA rank: {RANK}, alpha: {ALPHA}\")\n",
    "nnx.display(lora_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load SFT Checkpoint (if enabled) ===\n",
    "# This loads the LoRA weights from SFT warmup to give the model XML format knowledge\n",
    "\n",
    "def find_latest_sft_checkpoint(sft_dir: str) -> str | None:\n",
    "    \"\"\"Find the latest SFT checkpoint in the directory.\"\"\"\n",
    "    sft_path = Path(sft_dir)\n",
    "    if not sft_path.exists():\n",
    "        return None\n",
    "    \n",
    "    # Look for run directories\n",
    "    run_dirs = [d for d in sft_path.iterdir() if d.is_dir()]\n",
    "    if not run_dirs:\n",
    "        return None\n",
    "    \n",
    "    # Find latest checkpoint across all runs\n",
    "    latest_ckpt = None\n",
    "    latest_step = -1\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        # Look for step directories (numeric names)\n",
    "        for step_dir in run_dir.iterdir():\n",
    "            if step_dir.is_dir() and step_dir.name.isdigit():\n",
    "                step = int(step_dir.name)\n",
    "                model_params = step_dir / \"model_params\"\n",
    "                if model_params.exists() and step > latest_step:\n",
    "                    latest_step = step\n",
    "                    latest_ckpt = str(model_params)\n",
    "    \n",
    "    return latest_ckpt\n",
    "\n",
    "\n",
    "if LOAD_SFT_CHECKPOINT:\n",
    "    # Find checkpoint path\n",
    "    ckpt_path = SFT_CKPT_PATH or find_latest_sft_checkpoint(SFT_CKPT_DIR)\n",
    "    \n",
    "    if ckpt_path and os.path.exists(ckpt_path):\n",
    "        print(f\"Loading SFT checkpoint from: {ckpt_path}\")\n",
    "        \n",
    "        # Get abstract structure of LoRA params\n",
    "        abs_params = jax.tree.map(\n",
    "            lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "            nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        )\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpointer = ocp.StandardCheckpointer()\n",
    "        sft_lora_params = checkpointer.restore(ckpt_path, target=abs_params)\n",
    "        \n",
    "        # Update LoRA model with SFT weights\n",
    "        nnx.update(\n",
    "            lora_policy,\n",
    "            jax.tree.map(\n",
    "                lambda a, b: b,\n",
    "                nnx.state(lora_policy, nnx.LoRAParam),\n",
    "                sft_lora_params,\n",
    "            ),\n",
    "        )\n",
    "        print(\"SFT checkpoint loaded successfully!\")\n",
    "        print(\"Model now has XML format knowledge from SFT warmup.\")\n",
    "        wandb.run.summary[\"sft_checkpoint\"] = ckpt_path\n",
    "    else:\n",
    "        print(f\"WARNING: LOAD_SFT_CHECKPOINT is True but no checkpoint found!\")\n",
    "        print(f\"  Searched in: {SFT_CKPT_DIR}\")\n",
    "        print(f\"  Run 00_sft_warmup.ipynb first to create an SFT checkpoint.\")\n",
    "        print(\"  Continuing with fresh LoRA weights...\")\n",
    "else:\n",
    "    print(\"Skipping SFT checkpoint loading (LOAD_SFT_CHECKPOINT=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Define Reward Functions\n",
    "\n",
    "Tunix GRPO expects reward functions with signature:\n",
    "```python\n",
    "def reward_fn(prompts, completions, **kwargs) -> List[float]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward for having complete <reasoning>/<answer> format.\n",
    "    \n",
    "    Returns 3.0 if both tags present, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if has_valid_format(completion):\n",
    "            scores.append(3.0)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def match_format_approximately(prompts, completions, **kwargs):\n",
    "    \"\"\"Incremental reward for approaching correct format.\n",
    "    \n",
    "    Gives +0.5 for each tag present exactly once, -0.5 penalty if missing.\n",
    "    Range: -2.0 (no tags) to +2.0 (all 4 tags present once).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        # +0.5 for each correct tag present exactly once, -0.5 penalty if missing\n",
    "        score += 0.5 if completion.count(\"<reasoning>\") == 1 else -0.5\n",
    "        score += 0.5 if completion.count(\"</reasoning>\") == 1 else -0.5\n",
    "        score += 0.5 if completion.count(\"<answer>\") == 1 else -0.5\n",
    "        score += 0.5 if completion.count(\"</answer>\") == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Reward for correct answer with penalties for wrong answers.\n",
    "    \n",
    "    - 3.0 for exact match\n",
    "    - 1.5 for match after strip\n",
    "    - 0.5 for answer within 10% of correct\n",
    "    - 0.25 for answer within 20% of correct\n",
    "    - -1.0 for wrong answer (penalty)\n",
    "    - -0.5 for unparseable answer (penalty)\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion, true_answer in zip(completions, answer):\n",
    "        extracted = extract_tag(completion, \"answer\")\n",
    "        if not extracted:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        score = 0\n",
    "        # Exact match\n",
    "        if extracted == true_answer:\n",
    "            score += 3.0\n",
    "        # Match after stripping whitespace\n",
    "        elif extracted.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # Ratio-based partial credit / penalty\n",
    "            try:\n",
    "                ratio = float(extracted) / float(true_answer)\n",
    "                if 0.9 <= ratio <= 1.1:\n",
    "                    score += 0.5\n",
    "                elif 0.8 <= ratio <= 1.2:\n",
    "                    score += 0.25\n",
    "                else:\n",
    "                    score -= 1.0  # Penalize wrong answers\n",
    "            except (ValueError, ZeroDivisionError):\n",
    "                score -= 0.5  # Penalize unparseable\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Bonus for having the correct number appear anywhere in completion.\n",
    "    \n",
    "    Returns 1.0 if the answer number appears in the completion.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion, true_answer in zip(completions, answer):\n",
    "        norm_answer = normalize_math_answer(true_answer)\n",
    "        if norm_answer in completion:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Test reward functions\n",
    "test_prompts = [\"What is 2+2?\"]\n",
    "test_completions = [\"<reasoning>2+2=4</reasoning><answer>4</answer>\"]\n",
    "test_answers = [\"4\"]\n",
    "\n",
    "print(\"Testing reward functions:\")\n",
    "print(f\"  format_exact: {match_format_exactly(test_prompts, test_completions)}\")\n",
    "print(f\"  format_approx: {match_format_approximately(test_prompts, test_completions)}\")\n",
    "print(f\"  check_answer: {check_answer(test_prompts, test_completions, test_answers)}\")\n",
    "print(f\"  check_numbers: {check_numbers(test_prompts, test_completions, test_answers)}\")\n",
    "\n",
    "# Test with bad format (what the model was outputting)\n",
    "test_bad = [\"reasoning:\\n2+2=4\\nFinal answer: 4\"]\n",
    "print(\"\\nTesting with bad format (model's actual output):\")\n",
    "print(f\"  format_exact: {match_format_exactly(test_prompts, test_bad)}\")\n",
    "print(f\"  format_approx: {match_format_approximately(test_prompts, test_bad)}  # Now gives -2.0 penalty!\")\n",
    "\n",
    "# Test with wrong answer\n",
    "test_wrong = [\"<reasoning>2+2=5</reasoning><answer>5</answer>\"]\n",
    "print(\"\\nTesting with wrong answer:\")\n",
    "print(f\"  check_answer: {check_answer(test_prompts, test_wrong, test_answers)}  # Now gives -1.0 penalty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Load GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for math problems\n",
    "SYSTEM_PROMPT = \"\"\"You are a math tutor. Solve the problem step by step.\n",
    "Format your response EXACTLY as:\n",
    "<reasoning>your step-by-step solution</reasoning><answer>final numerical answer only</answer>\"\"\"\n",
    "\n",
    "TEMPLATE = \"<start_of_turn>user\\n{system_prompt}\\n\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "\n",
    "def extract_hash_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract final answer from GSM8K format.\n",
    "    \n",
    "    GSM8K answers end with #### followed by the numeric answer.\n",
    "    \"\"\"\n",
    "    if \"####\" in answer_text:\n",
    "        return answer_text.split(\"####\")[-1].strip()\n",
    "    return answer_text.strip()\n",
    "\n",
    "\n",
    "def format_gsm8k_example(example):\n",
    "    \"\"\"Format a GSM8K example for GRPO training.\"\"\"\n",
    "    question = example[\"question\"]\n",
    "    answer = extract_hash_answer(example[\"answer\"])\n",
    "    \n",
    "    prompt = TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"prompts\": prompt,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load GSM8K dataset from HuggingFace (more reliable than tfds)\n",
    "print(\"Loading GSM8K dataset from HuggingFace...\")\n",
    "gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "gsm8k_test = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "print(f\"Train examples: {len(gsm8k_train)}\")\n",
    "print(f\"Test examples: {len(gsm8k_test)}\")\n",
    "\n",
    "# Convert to list and format (HuggingFace returns strings directly)\n",
    "train_data = [\n",
    "    format_gsm8k_example({\"question\": ex[\"question\"], \n",
    "                          \"answer\": ex[\"answer\"]})\n",
    "    for ex in tqdm(gsm8k_train, desc=\"Formatting train data\")\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    format_gsm8k_example({\"question\": ex[\"question\"], \n",
    "                          \"answer\": ex[\"answer\"]})\n",
    "    for ex in tqdm(gsm8k_test, desc=\"Formatting test data\")\n",
    "]\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample formatted data:\")\n",
    "pprint(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grain MapDataset from our formatted data\n",
    "# (Tunix expects grain.MapDataset, not plain Python lists)\n",
    "train_grain = (\n",
    "    grain.MapDataset.source(train_data)\n",
    "    .shuffle(seed=42)\n",
    "    .batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n",
    ")\n",
    "\n",
    "test_grain = (\n",
    "    grain.MapDataset.source(test_data)\n",
    "    .batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
    ")\n",
    "\n",
    "# Split train into train/val\n",
    "split_idx = int(len(train_grain) * TRAIN_FRACTION)\n",
    "if TRAIN_FRACTION == 1.0:\n",
    "    train_dataset = train_grain.repeat(NUM_EPOCHS)\n",
    "    val_dataset = None\n",
    "else:\n",
    "    train_dataset = train_grain[:split_idx].repeat(NUM_EPOCHS)\n",
    "    val_dataset = train_grain[split_idx:]\n",
    "\n",
    "test_dataset = test_grain\n",
    "\n",
    "print(f\"Train batches: {len(train_dataset)}\")\n",
    "print(f\"Val batches: {len(val_dataset) if val_dataset else 0}\")\n",
    "print(f\"Test batches: {len(test_dataset)}\")\n",
    "\n",
    "# Show first batch\n",
    "print(\"\\nFirst training batch:\")\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Configure Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW optimizer with warmup cosine decay schedule\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Add gradient clipping\n",
    "if MAX_GRAD_NORM is not None:\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
    "        optimizer,\n",
    "    )\n",
    "\n",
    "print(\"Optimizer configured.\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Gradient clipping: {MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Configure GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics logging options\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=TENSORBOARD_DIR,\n",
    "    flush_every_n_steps=20,\n",
    ")\n",
    "\n",
    "# Checkpointing options\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    max_to_keep=3,\n",
    "    save_interval_steps=EVAL_EVERY_N_STEPS,\n",
    ")\n",
    "\n",
    "# RL Cluster configuration\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',  # Use vanilla for single GPU\n",
    "    offload_to_cpu=True,       # Enable CPU offloading for RTX 3090 memory\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=[1, 106],  # Gemma EOS tokens - critical for proper generation stopping\n",
    "    ),\n",
    ")\n",
    "\n",
    "# GRPO algorithm configuration\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,  # Generate N responses per prompt\n",
    "    num_iterations=NUM_ITERATIONS,    # Optimization iterations per batch\n",
    "    beta=BETA,                        # KL divergence penalty\n",
    "    epsilon=EPSILON,                  # Clipping range\n",
    ")\n",
    "\n",
    "print(\"GRPO configuration complete.\")\n",
    "print(f\"  Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"  KL beta: {BETA}\")\n",
    "print(f\"  Clip epsilon: {EPSILON}\")\n",
    "print(f\"  CPU offloading: enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Initialize RLCluster and GRPOLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RL Cluster\n",
    "# Note: Using base_model as reference enables backbone sharing with lora_policy\n",
    "print(\"Creating RL cluster...\")\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=base_model,  # Shares backbone with lora_policy for memory efficiency\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "print(\"RL cluster created.\")\n",
    "\n",
    "# Create GRPO Trainer\n",
    "print(\"Creating GRPO learner...\")\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    algo_config=grpo_config,\n",
    ")\n",
    "print(\"GRPO learner created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15b: Periodic Completion Logging\n",
    "\n",
    "Register a callback to print sample completions every N steps so we can see what format the model is outputting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periodic completion logging\n",
    "def log_completions(metrics_buffer):\n",
    "    \"\"\"Print sample prompts and completions every N steps.\"\"\"\n",
    "    step = metrics_buffer.global_steps\n",
    "    if step % 50 != 0:\n",
    "        return\n",
    "\n",
    "    metrics = metrics_buffer.metrics\n",
    "    if \"completions\" not in metrics:\n",
    "        return\n",
    "\n",
    "    completions, _ = metrics[\"completions\"]\n",
    "    prompts, _ = metrics.get(\"prompts\", ([], None))\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Step {step} - Sample Prompts & Completions\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for i in range(min(2, len(completions))):\n",
    "        if prompts and i < len(prompts):\n",
    "            print(f\"\\n[Prompt {i+1}]\")\n",
    "            print(prompts[i])\n",
    "        print(f\"\\n[Completion {i+1}]\")\n",
    "        print(completions[i][:800])\n",
    "        if len(completions[i]) > 800:\n",
    "            print(f\"... [truncated, {len(completions[i])} total chars]\")\n",
    "    print()\n",
    "\n",
    "rl_cluster.with_external_metrics_logger(log_completions)\n",
    "print(\"Completion logging registered (every 50 steps).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint preservation callback - saves best and epoch-end checkpoints\n",
    "# These won't be deleted by Orbax's max_to_keep policy\n",
    "\n",
    "_ckpt_state = {\n",
    "    \"best_reward\": float(\"-inf\"),\n",
    "    \"best_step\": None,\n",
    "}\n",
    "\n",
    "def preserve_checkpoints(metrics_buffer):\n",
    "    \"\"\"Preserve best and epoch-end checkpoints, log to wandb.\"\"\"\n",
    "    step = metrics_buffer.global_steps\n",
    "    metrics = metrics_buffer.metrics\n",
    "\n",
    "    # Calculate total reward from metrics\n",
    "    total_reward = 0.0\n",
    "    reward_dict = {}\n",
    "    for key in [\"match_format_exactly\", \"match_format_approximately\",\n",
    "                \"check_answer\", \"check_numbers\"]:\n",
    "        if key in metrics:\n",
    "            val, _ = metrics[key]\n",
    "            if isinstance(val, (int, float)):\n",
    "                reward_val = float(val)\n",
    "            elif hasattr(val, \"mean\"):\n",
    "                reward_val = float(val.mean())\n",
    "            else:\n",
    "                reward_val = 0.0\n",
    "            total_reward += reward_val\n",
    "            reward_dict[f\"reward/{key}\"] = reward_val\n",
    "\n",
    "    # Log to wandb\n",
    "    log_dict = {\n",
    "        \"step\": step,\n",
    "        \"reward/total\": total_reward,\n",
    "        **reward_dict,\n",
    "    }\n",
    "    \n",
    "    # Add other metrics\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, tuple) and len(v) == 2:\n",
    "            val, _ = v\n",
    "            if isinstance(val, (int, float)):\n",
    "                log_dict[k] = val\n",
    "            elif hasattr(val, \"mean\"):\n",
    "                log_dict[k] = float(val.mean())\n",
    "    \n",
    "    wandb.log(log_dict, step=step)\n",
    "\n",
    "    # Check for new best\n",
    "    if total_reward > _ckpt_state[\"best_reward\"]:\n",
    "        _ckpt_state[\"best_reward\"] = total_reward\n",
    "        _ckpt_state[\"best_step\"] = step\n",
    "        _copy_checkpoint(step, \"best\")\n",
    "        print(f\"[Checkpoint] New best at step {step}: reward={total_reward:.3f}\")\n",
    "        wandb.run.summary[\"best_step\"] = step\n",
    "        wandb.run.summary[\"best_reward\"] = total_reward\n",
    "\n",
    "    # Epoch-end preservation (steps_per_epoch = NUM_BATCHES * TRAIN_FRACTION)\n",
    "    steps_per_epoch = int(NUM_BATCHES * TRAIN_FRACTION)\n",
    "    if step > 0 and step % steps_per_epoch == 0:\n",
    "        epoch = step // steps_per_epoch\n",
    "        _copy_checkpoint(step, f\"epoch_{epoch}\")\n",
    "        print(f\"[Checkpoint] Saved epoch {epoch} checkpoint at step {step}\")\n",
    "\n",
    "\n",
    "def _copy_checkpoint(step: int, name: str):\n",
    "    \"\"\"Copy a checkpoint to preserved directory.\"\"\"\n",
    "    src = Path(CKPT_DIR) / \"actor\" / str(step)\n",
    "    dst = Path(CKPT_DIR) / \"preserved\" / name\n",
    "\n",
    "    if not src.exists():\n",
    "        # Checkpoint may not be saved yet, try nearby steps\n",
    "        for offset in [0, -1, 1, -2, 2, -5, 5, -10, 10]:\n",
    "            candidate = Path(CKPT_DIR) / \"actor\" / str(step + offset)\n",
    "            if candidate.exists():\n",
    "                src = candidate\n",
    "                break\n",
    "        else:\n",
    "            print(f\"[Checkpoint] Warning: No checkpoint found near step {step}\")\n",
    "            return\n",
    "\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists():\n",
    "        shutil.rmtree(dst)\n",
    "    shutil.copytree(src, dst)\n",
    "    print(f\"[Checkpoint] Copied {src.name} -> preserved/{name}\")\n",
    "\n",
    "\n",
    "rl_cluster.with_external_metrics_logger(preserve_checkpoints)\n",
    "print(\"Checkpoint preservation callback registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: TensorBoard Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load TensorBoard extension for monitoring\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {TENSORBOARD_DIR} --port=0\n",
    "# TENSORBOARD_DIR\n",
    "f\"tensorboard --logdir {TENSORBOARD_DIR} --port=6006\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 17: Run GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training!\n",
    "print(\"Starting GRPO training...\")\n",
    "print(f\"  Total steps: {MAX_STEPS}\")\n",
    "print(f\"  Batch size: {TRAIN_MICRO_BATCH_SIZE}\")\n",
    "print(f\"  Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print()\n",
    "\n",
    "with mesh:\n",
    "    grpo_trainer.train(train_dataset)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sampler for inference (wandb already initialized in Cell 4)\n",
    "sampler = create_sampler(\n",
    "    lora_policy,\n",
    "    tokenizer,\n",
    "    model_config,\n",
    "    max_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    ")\n",
    "print(\"Sampler created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX's monitoring callbacks need wandb initialized (disabled mode = no actual logging)\n",
    "if wandb.run is None:\n",
    "    wandb.init(mode=\"disabled\")\n",
    "\n",
    "# Create sampler for inference\n",
    "sampler = create_sampler(\n",
    "    lora_policy,\n",
    "    tokenizer,\n",
    "    model_config,\n",
    "    max_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    ")\n",
    "print(\"Sampler created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model on a few examples\n",
    "print(\"Testing trained model...\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is 15 + 27?\",\n",
    "    \"If Mary has 5 apples and gives 2 to John, how many apples does Mary have?\",\n",
    "    \"A store sells 3 books for $12. How much does one book cost?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    prompt = TEMPLATE.format(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    # Generate using greedy config for deterministic evaluation\n",
    "    result = sampler(\n",
    "        input_strings=[prompt],\n",
    "        max_generation_steps=TOTAL_GENERATION_STEPS,\n",
    "        **GENERATION_CONFIGS[\"greedy\"],  # Use greedy for eval (deterministic)\n",
    "        echo=False,\n",
    "        eos_tokens=[1, 106],  # EOS tokens for Gemma 3\n",
    "    )\n",
    "    \n",
    "    response = result.text[0]\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nCheckpoints saved to:\", CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up intermediate checkpoints (optional)\n",
    "# Uncomment to remove intermediate files\n",
    "# shutil.rmtree(INTERMEDIATE_CKPT_DIR, ignore_errors=True)\n",
    "# print(\"Intermediate checkpoints cleaned up.\")\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"WandB run finished.\")\n",
    "\n",
    "# Free GPU memory\n",
    "del grpo_trainer\n",
    "del rl_cluster\n",
    "del lora_policy\n",
    "del base_model\n",
    "gc.collect()\n",
    "print(\"Memory freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up intermediate checkpoints (optional)\n",
    "# Uncomment to remove intermediate files\n",
    "# shutil.rmtree(INTERMEDIATE_CKPT_DIR, ignore_errors=True)\n",
    "# print(\"Intermediate checkpoints cleaned up.\")\n",
    "\n",
    "# Free GPU memory\n",
    "del grpo_trainer\n",
    "del rl_cluster\n",
    "del lora_policy\n",
    "del base_model\n",
    "gc.collect()\n",
    "print(\"Memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunix-hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
